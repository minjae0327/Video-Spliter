{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import cv2\n",
    "import easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    }
   ],
   "source": [
    "reader = easyocr.Reader(['en'], gpu=True) # this needs to run only once to load the model into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', '11')\n",
      "('10', '11')\n"
     ]
    }
   ],
   "source": [
    "def extract_task_number(text):\n",
    "    # 괄호 안의 두 숫자를 각각 캡처하는 정규식\n",
    "    match = re.search(r'\\((\\d+)/(\\d+)\\)', text)\n",
    "    if match:\n",
    "        task_number = match.group(1)\n",
    "        last_instruction = match.group(2)\n",
    "        return task_number, last_instruction\n",
    "    return None, None\n",
    "\n",
    "# 테스트 예제\n",
    "text1 = \"TASK : 32-11-46-400-001-A (1/11)\"\n",
    "text2 = \"32-11-46-400-001-A (10/11)\"\n",
    "\n",
    "print(extract_task_number(text1))  # 출력: 1\n",
    "print(extract_task_number(text2))  # 출력: 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = reader.readtext('frame1.png', detail=0)\n",
    "# result = extract_task_number(text[1])\n",
    "# result\n",
    "not(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_output(predicted_text, timestamp):\n",
    "    \"\"\"OCR 결과가 이전과 다를 경우 변경된 시점 저장\"\"\"\n",
    "    global previous_text, timestamps\n",
    "\n",
    "    # OCR 결과가 이전과 다를 경우\n",
    "    if predicted_text != previous_text:\n",
    "        print(f\"[{timestamp} ms] OCR 변경 감지: {predicted_text}\")\n",
    "        timestamps.append((timestamp, predicted_text))  # 변경된 시간 저장\n",
    "        previous_text = predicted_text  # 이전 텍스트 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitVideo():\n",
    "    def __init__(self, video_path, save_path):\n",
    "        self.video_path = video_path\n",
    "        self.save_path = save_path\n",
    "        self.previous_text = \"\"\n",
    "        self.timestamps = []\n",
    "        \n",
    "        \n",
    "    def __call__(self):\n",
    "        # 비디오 캡처 객체 생성\n",
    "        cap = cv2.VideoCapture(self.video_path)\n",
    "        frame_count = 0\n",
    "        roi = None\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame = cv2.resize(frame, (1980, 1080))\n",
    "            \n",
    "            if roi is None:\n",
    "                roi = cv2.selectROI(\"Select ROI\", frame, False, False)\n",
    "                cv2.destroyWindow(\"Select ROI\")\n",
    "                x, y, w, h = roi\n",
    "\n",
    "            if frame_count % 6 == 0:\n",
    "                # 특정 영역 크롭\n",
    "                cropped_frame = frame[y:y+h, x:x+w]\n",
    "\n",
    "                # OCR 모델 예측\n",
    "                inst_number = reader.readtext(cropped_frame, detail=0)\n",
    "                inst_number = inst_number[1]\n",
    "                inst_number = inst_number.split(\"/\")[0]\n",
    "                inst_number = inst_number[-1]\n",
    "\n",
    "                # 현재 동영상 시간(ms) 가져오기\n",
    "                timestamp = int(cap.get(cv2.CAP_PROP_POS_MSEC))\n",
    "\n",
    "                # OCR 결과 비교 및 시간 기록\n",
    "                self.record_timestemp(inst_number, timestamp)\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        \n",
    "    def record_timestemp(self, predicted_text, timestamp):\n",
    "        \"\"\"OCR 결과가 이전과 다를 경우 변경된 시점 저장\"\"\"\n",
    "\n",
    "        # OCR 결과가 이전과 다를 경우\n",
    "        if predicted_text != self.previous_text:\n",
    "            print(f\"[{timestamp} ms] OCR 변경 감지: {predicted_text}\")\n",
    "            self.timestamps.append(timestamp, predicted_text)\n",
    "            self.previous_text = predicted_text  # 이전 텍스트 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 ms] OCR 변경 감지: 1\n",
      "[22800 ms] OCR 변경 감지: 2\n",
      "[57900 ms] OCR 변경 감지: 3\n",
      "[92700 ms] OCR 변경 감지: 4\n",
      "[106700 ms] OCR 변경 감지: 5\n",
      "[122700 ms] OCR 변경 감지: 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m cropped_frame \u001b[38;5;241m=\u001b[39m frame[y:y\u001b[38;5;241m+\u001b[39mh, x:x\u001b[38;5;241m+\u001b[39mw]\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# OCR 모델 예측\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m inst_number \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mreadtext(cropped_frame, detail\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     31\u001b[0m inst_number \u001b[38;5;241m=\u001b[39m inst_number[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     32\u001b[0m inst_number \u001b[38;5;241m=\u001b[39m inst_number\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\minja\\anaconda3\\envs\\easyOCR\\Lib\\site-packages\\easyocr\\easyocr.py:468\u001b[0m, in \u001b[0;36mReader.readtext\u001b[1;34m(self, image, decoder, beamWidth, batch_size, workers, allowlist, blocklist, detail, rotation_info, paragraph, min_size, contrast_ths, adjust_contrast, filter_ths, text_threshold, low_text, link_threshold, canvas_size, mag_ratio, slope_ths, ycenter_ths, height_ths, width_ths, y_ths, x_ths, add_margin, threshold, bbox_min_score, bbox_min_size, max_candidates, output_format)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;66;03m# get the 1st result from hor & free list as self.detect returns a list of depth 3\u001b[39;00m\n\u001b[0;32m    467\u001b[0m horizontal_list, free_list \u001b[38;5;241m=\u001b[39m horizontal_list[\u001b[38;5;241m0\u001b[39m], free_list[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 468\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecognize(img_cv_grey, horizontal_list, free_list,\\\n\u001b[0;32m    469\u001b[0m                         decoder, beamWidth, batch_size,\\\n\u001b[0;32m    470\u001b[0m                         workers, allowlist, blocklist, detail, rotation_info,\\\n\u001b[0;32m    471\u001b[0m                         paragraph, contrast_ths, adjust_contrast,\\\n\u001b[0;32m    472\u001b[0m                         filter_ths, y_ths, x_ths, \u001b[38;5;28;01mFalse\u001b[39;00m, output_format)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\minja\\anaconda3\\envs\\easyOCR\\Lib\\site-packages\\easyocr\\easyocr.py:384\u001b[0m, in \u001b[0;36mReader.recognize\u001b[1;34m(self, img_cv_grey, horizontal_list, free_list, decoder, beamWidth, batch_size, workers, allowlist, blocklist, detail, rotation_info, paragraph, contrast_ths, adjust_contrast, filter_ths, y_ths, x_ths, reformat, output_format)\u001b[0m\n\u001b[0;32m    382\u001b[0m     f_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    383\u001b[0m     image_list, max_width \u001b[38;5;241m=\u001b[39m get_image_list(h_list, f_list, img_cv_grey, model_height \u001b[38;5;241m=\u001b[39m imgH)\n\u001b[1;32m--> 384\u001b[0m     result0 \u001b[38;5;241m=\u001b[39m get_text(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharacter, imgH, \u001b[38;5;28mint\u001b[39m(max_width), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecognizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverter, image_list,\\\n\u001b[0;32m    385\u001b[0m                   ignore_char, decoder, beamWidth, batch_size, contrast_ths, adjust_contrast, filter_ths,\\\n\u001b[0;32m    386\u001b[0m                   workers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    387\u001b[0m     result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m result0\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bbox \u001b[38;5;129;01min\u001b[39;00m free_list:\n",
      "File \u001b[1;32mc:\\Users\\minja\\anaconda3\\envs\\easyOCR\\Lib\\site-packages\\easyocr\\recognition.py:206\u001b[0m, in \u001b[0;36mget_text\u001b[1;34m(character, imgH, imgW, recognizer, converter, image_list, ignore_char, decoder, beamWidth, batch_size, contrast_ths, adjust_contrast, filter_ths, workers, device)\u001b[0m\n\u001b[0;32m    201\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m    202\u001b[0m     test_data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    203\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(workers), collate_fn\u001b[38;5;241m=\u001b[39mAlignCollate_normal, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# predict first round\u001b[39;00m\n\u001b[1;32m--> 206\u001b[0m result1 \u001b[38;5;241m=\u001b[39m recognizer_predict(recognizer, converter, test_loader,batch_max_length,\\\n\u001b[0;32m    207\u001b[0m                              ignore_idx, char_group_idx, decoder, beamWidth, device \u001b[38;5;241m=\u001b[39m device)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# predict second round\u001b[39;00m\n\u001b[0;32m    210\u001b[0m low_confident_idx \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i,item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(result1) \u001b[38;5;28;01mif\u001b[39;00m (item[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m contrast_ths)]\n",
      "File \u001b[1;32mc:\\Users\\minja\\anaconda3\\envs\\easyOCR\\Lib\\site-packages\\easyocr\\recognition.py:111\u001b[0m, in \u001b[0;36mrecognizer_predict\u001b[1;34m(model, converter, test_loader, batch_max_length, ignore_idx, char_group_idx, decoder, beamWidth, device)\u001b[0m\n\u001b[0;32m    108\u001b[0m length_for_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mIntTensor([batch_max_length] \u001b[38;5;241m*\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    109\u001b[0m text_for_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(batch_size, batch_max_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 111\u001b[0m preds \u001b[38;5;241m=\u001b[39m model(image, text_for_pred)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Select max probabilty (greedy decoding) then decode index to character\u001b[39;00m\n\u001b[0;32m    114\u001b[0m preds_size \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mIntTensor([preds\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)] \u001b[38;5;241m*\u001b[39m batch_size)\n",
      "File \u001b[1;32mc:\\Users\\minja\\anaconda3\\envs\\easyOCR\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\minja\\anaconda3\\envs\\easyOCR\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\minja\\anaconda3\\envs\\easyOCR\\Lib\\site-packages\\easyocr\\model\\vgg_model.py:30\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, input, text)\u001b[0m\n\u001b[0;32m     27\u001b[0m visual_feature \u001b[38;5;241m=\u001b[39m visual_feature\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Sequence modeling stage \"\"\"\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m contextual_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSequenceModeling(visual_feature)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Prediction stage \"\"\"\u001b[39;00m\n\u001b[0;32m     33\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPrediction(contextual_feature\u001b[38;5;241m.\u001b[39mcontiguous())\n",
      "File \u001b[1;32mc:\\Users\\minja\\anaconda3\\envs\\easyOCR\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\minja\\anaconda3\\envs\\easyOCR\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\minja\\anaconda3\\envs\\easyOCR\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\minja\\anaconda3\\envs\\easyOCR\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\minja\\anaconda3\\envs\\easyOCR\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\minja\\anaconda3\\envs\\easyOCR\\Lib\\site-packages\\easyocr\\model\\modules.py:98\u001b[0m, in \u001b[0;36mBidirectionalLSTM.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m: \u001b[38;5;66;03m# quantization doesn't work with this \u001b[39;00m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m recurrent, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(\u001b[38;5;28minput\u001b[39m)  \u001b[38;5;66;03m# batch_size x T x input_size -> batch_size x T x (2*hidden_size)\u001b[39;00m\n\u001b[0;32m     99\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(recurrent)  \u001b[38;5;66;03m# batch_size x T x output_size\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\minja\\anaconda3\\envs\\easyOCR\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\minja\\anaconda3\\envs\\easyOCR\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\minja\\anaconda3\\envs\\easyOCR\\Lib\\site-packages\\torch\\ao\\nn\\quantized\\dynamic\\modules\\rnn.py:656\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    654\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_packed(\u001b[38;5;28minput\u001b[39m, hx)\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 656\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_tensor(\u001b[38;5;28minput\u001b[39m, hx)\n",
      "File \u001b[1;32mc:\\Users\\minja\\anaconda3\\envs\\easyOCR\\Lib\\site-packages\\torch\\ao\\nn\\quantized\\dynamic\\modules\\rnn.py:602\u001b[0m, in \u001b[0;36mLSTM.forward_tensor\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    599\u001b[0m sorted_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    600\u001b[0m unsorted_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_impl(\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28minput\u001b[39m, hx, batch_sizes, max_batch_size, sorted_indices\n\u001b[0;32m    604\u001b[0m )\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hidden, unsorted_indices)\n",
      "File \u001b[1;32mc:\\Users\\minja\\anaconda3\\envs\\easyOCR\\Lib\\site-packages\\torch\\ao\\nn\\quantized\\dynamic\\modules\\rnn.py:561\u001b[0m, in \u001b[0;36mLSTM.forward_impl\u001b[1;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[0;32m    559\u001b[0m _all_params \u001b[38;5;241m=\u001b[39m [m\u001b[38;5;241m.\u001b[39mparam \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_all_weight_values]\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 561\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mquantized_lstm(\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    563\u001b[0m         hx,\n\u001b[0;32m    564\u001b[0m         _all_params,\n\u001b[0;32m    565\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    566\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    567\u001b[0m         \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout),\n\u001b[0;32m    568\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[0;32m    569\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m    570\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first,\n\u001b[0;32m    571\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m    572\u001b[0m         use_dynamic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m     )\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    575\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mquantized_lstm(\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    577\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    586\u001b[0m         use_dynamic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    587\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\minja\\anaconda3\\envs\\easyOCR\\Lib\\site-packages\\torch\\_ops.py:1123\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[1;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 비디오 파일 경로\n",
    "video_path = \"Installation of the Lockstay Actuator Hoses.mp4\"\n",
    "\n",
    "# OCR 결과 추적 변수\n",
    "previous_text = None  # 이전 OCR 결과\n",
    "timestamps = []  # 변경된 시점 기록\n",
    "\n",
    "# 비디오 캡처 객체 생성\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_count = 0\n",
    "roi = None\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame = cv2.resize(frame, (1980, 1080))\n",
    "    \n",
    "    if roi is None:\n",
    "        roi = cv2.selectROI(\"Select ROI\", frame, False, False)\n",
    "        cv2.destroyWindow(\"Select ROI\")\n",
    "        x, y, w, h = roi\n",
    "\n",
    "    if frame_count % 6 == 0:\n",
    "        # 특정 영역 크롭\n",
    "        cropped_frame = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # OCR 모델 예측\n",
    "        inst_number = reader.readtext(cropped_frame, detail=0)\n",
    "        inst_number = inst_number[1]\n",
    "        inst_number = inst_number.split(\"/\")[0]\n",
    "        inst_number = inst_number[-1]\n",
    "\n",
    "        # 현재 동영상 시간(ms) 가져오기\n",
    "        timestamp = int(cap.get(cv2.CAP_PROP_POS_MSEC))\n",
    "\n",
    "        # OCR 결과 비교 및 시간 기록\n",
    "        decode_output(inst_number, timestamp)\n",
    "    \n",
    "    frame_count += 1\n",
    "    \n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# 최종 기록된 시간 출력\n",
    "print(\"\\n=== OCR 변경 시점 기록 ===\")\n",
    "for t, text in timestamps:\n",
    "    print(f\"{t} ms: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easyOCR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
